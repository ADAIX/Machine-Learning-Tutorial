{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooking a simple neural network library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredients\n",
    "\n",
    "- `numpy`\n",
    "- [a loss function](#Loss-function)\n",
    "- [some layers](#Layers)\n",
    "- [a neural net](#Neural-network)\n",
    "- [an optimizer](#Optimizer)\n",
    "- [a batch data provider](#Batch-generator)\n",
    "- [a training routine](#Training)\n",
    "- \\+ an example\n",
    "\n",
    "Hopefully by the end of this tutorial you will have an understanding of the building blocks needed for training (deep) neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreword\n",
    "\n",
    "We will purely rely on numpy for this tutorial. Make sure to import it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Oriented Python\n",
    "\n",
    "Object-oriented Python, a.k.a _classes_, will be used intensively in this tutorial.  \n",
    "For those not familiar with Python classes, know that you will only be required to write some definitions and Python code **within the** class **methods** and **not** actually **write any class**.  \n",
    "\n",
    "If you want to know more about Python classes, here is a step by step [tutorial](https://aboucaud.github.io/slides/2016/python-classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type hints\n",
    "\n",
    "This notebook uses a feature from Python 3 called ***type hints*** or ***type annotations*** (see [PEP 0526](https://www.python.org/dev/peps/pep-0526/)). This acts like optional static typing since Python will still run if the type does not match, but has two main advantages IMO:\n",
    "- make sure you understand what you're doing\n",
    "- act like documentation for an external user\n",
    "\n",
    "The types for the base Python objects (lists, dicts, iterables) can be found in the [`typing` library](https://docs.python.org/3/library/typing.html).\n",
    "For instance, here are all the needed imports for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (Dict, Tuple, Callable, \n",
    "                    Sequence, Iterator, NamedTuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any other Python object can serve as a type. We will use the `numpy.ndarray` in this tutorial to mock a tensor. We thus create a `Tensor` object to use as type hint throughout the code, and an object `Func` for a function that acts element-wise on a tensor and returns a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray as Tensor\n",
    "\n",
    "Func = Callable[[Tensor], Tensor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the type\n",
    "\n",
    "If type hints are optional for Python, they can still be used to actually check the consistency of the code. For this task, there is a module called [`mypy`](https://github.com/python/mypy) that can be used (not in this tutorial). \n",
    "\n",
    "Check out the [doc](http://mypy-lang.org/) if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "A loss function measures how good our predictions are,\n",
    "we can use this to adjust the parameters of our network\n",
    "\n",
    "Here is generic loss class. It implements the computation of **the loss** from the true label and the predicted one, as well as **the gradient of the loss** for the backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def grad(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice #1 - mean square error loss\n",
    "\n",
    "***3 min*** - *Implement the `MeanSquareError` class*\n",
    "\n",
    "For info, the loss function is\n",
    "\n",
    "$$MSE(y_{true}, y_{pred}) = \\sum \\left(y_{pred} - y_{true}\\right) ^ 2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquareError(Loss):\n",
    "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
    "        return np.sum((predicted - actual) ** 2)\n",
    "    \n",
    "    def grad(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
    "        return 2 * (predicted - actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "Our neural nets will be made up of layers.\n",
    "Each layer needs to pass its inputs forward\n",
    "and propagate gradients backward. For example,\n",
    "a neural net might look like\n",
    "inputs -> Linear -> Tanh -> Linear -> output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self) -> None:\n",
    "        self.params: Dict[str, Tensor] = {}\n",
    "        self.grads: Dict[str, Tensor] = {}\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad: Tensor) -> Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice #2 - linear layer\n",
    "\n",
    "***5 min*** - *Implement the `forward` and `backward` methods of the linear layer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"\n",
    "    Inputs are of size (batch_size, input_size)\n",
    "    Outputs are of size (batch_size, output_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, output_size: int) -> None:\n",
    "        # Inherit from base class Layer\n",
    "        super().__init__()\n",
    "        # Initialize the weights and bias with random values\n",
    "        self.params[\"w\"] = np.random.randn(input_size, output_size)\n",
    "        self.params[\"b\"] = np.random.randn(output_size)\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        inputs shape is (batch_size, input_size)\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        # Compute here the feed forward pass\n",
    "        # or self.params['w'].T @ inputs.T + + self.params['b']\n",
    "        return inputs @ self.params['w'] + self.params['b'] \n",
    "        \n",
    "\n",
    "    def backward(self, grad: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        grad shape is (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Compute here the gradient parameters for the layer\n",
    "        self.grads[\"w\"] = grad.T @ self.inputs\n",
    "        self.grads[\"b\"] = np.sum(grad , axis=0)\n",
    "        # Compute here the feed backward pass\n",
    "        return grad @ self.params['w'].T    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    \"\"\"\n",
    "    An activation layer just applies a function\n",
    "    elementwise to its inputs\n",
    "    \"\"\"\n",
    "    def __init__(self, f: Func, f_prime: Func) -> None:\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.f_prime = f_prime\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        self.inputs = inputs\n",
    "        return self.f(inputs)\n",
    "\n",
    "    def backward(self, grad: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        if y = f(x) and x = g(z)\n",
    "        then dy/dz = f'(x) * g'(z)\n",
    "        \"\"\"\n",
    "        return self.f_prime(self.inputs) * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice #3 - tanh and sigmoid\n",
    "\n",
    "***5 min*** - *Implement the hyperbolic tangent and sigmoid layers and their derivatives.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x: Tensor) -> Tensor:\n",
    "    # Write here the tanh function\n",
    "    return np.tanh(x) \n",
    "\n",
    "def tanh_prime(x: Tensor) -> Tensor:\n",
    "    # Write here the derivative of the tanh\n",
    "    y = tanh(x)\n",
    "    return 1 - y ** 2\n",
    "\n",
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__(tanh, tanh_prime)\n",
    "        \n",
    "\n",
    "def sigmoid(x: Tensor) -> Tensor:\n",
    "    # Write here the sigmoid function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x: Tensor) -> Tensor:\n",
    "    # Write here the derivative of the sigmoid\n",
    "    y = sigmoid(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__(sigmoid, sigmoid_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "A neural net is a collection of layers and should behave closely with a `forward` and a `backward` pass.\n",
    "\n",
    "In addition, we add a method `params_and_grads` that will be used by the optimizer to update the values of the weights and bias of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, layers: Sequence[Layer]) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        The forward pass takes the layers in order\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, grad: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        The backward pass is the other way around\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def params_and_grads(self) -> Iterator[Tuple[Tensor, Tensor]]:\n",
    "        for layer in self.layers:\n",
    "            for name, param in layer.params.items():\n",
    "                grad = layer.grads[name]\n",
    "                yield param, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "The role of the optimizer is to adjust the network parameters based on the gradients computed during backpropagation.\n",
    "\n",
    "The main attribute of an optimizer is the _learning rate_ (a.k.a. `lr`), which defines the size of the jump taken in the direction of the gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def step(self, net: NeuralNet) -> None:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice #4 - Stochastic Gradient Descent\n",
    "\n",
    "***2 min*** - write the optimizer step\n",
    "\n",
    "Here we have a very basic implementation of a _Stochastic Gradient Descent_ (a.k.a. `SGD`). \n",
    "\n",
    "The step that needs to be written iterates over the neural network layers and updates the layers parameters in the direction _opposite_ to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr: float = 0.01) -> None:\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, net: NeuralNet) -> None:\n",
    "        for param, grad in net.params_and_grads(): \n",
    "            # Write here the parameters update\n",
    "            param -= self.lr * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch generator\n",
    "\n",
    "It can be costly to compute the gradients and update the weights after every entry of the training dataset. In order to minimize such computational cost, the inputs of the network are traditionally fed in batches and the gradients are thus averages over those batches of data.\n",
    "\n",
    "A batch size of 32 is a default in multiple training sets. Some recent [study](https://arxiv.org/abs/1804.07612) claims this number is the perfect balance between computing efficiency and training stability.\n",
    "\n",
    "During an epoch the network will iterate over the whole dataset. Adding some shuffling in the process ensures the batches are not fed exactly in the same order at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = NamedTuple(\"Batch\", [(\"inputs\", Tensor), (\"targets\", Tensor)])\n",
    "\n",
    "\n",
    "class DataIterator:\n",
    "    def __call__(self, inputs: Tensor, targets: Tensor) -> Iterator[Batch]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class BatchIterator(DataIterator):\n",
    "    def __init__(self, batch_size: int = 32, shuffle: bool = True) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __call__(self, inputs: Tensor, targets: Tensor) -> Iterator[Batch]:\n",
    "        starts = np.arange(0, len(inputs), self.batch_size)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(starts)\n",
    "\n",
    "        for start in starts:\n",
    "            end = start + self.batch_size\n",
    "            batch_inputs = inputs[start:end]\n",
    "            batch_targets = targets[start:end]\n",
    "            yield Batch(batch_inputs, batch_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The training routine uses all objects defined above and executes actions **in the right order** to train the neural network.\n",
    "\n",
    "The dataset being usually small with respect to the number of free parameters of the neural net, going through the dataset multiple times during the training is a necessity. This corresponds to the number of epochs, which has to be specified.\n",
    "\n",
    "### Exercise #5 - build the training routine\n",
    "\n",
    "***5 min*** - write the sequential steps needed for training at each epoch\n",
    "\n",
    "_Hints_:\n",
    "- feed forward\n",
    "- compute the loss and the gradients\n",
    "- feed backwards\n",
    "- update the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net: NeuralNet, inputs: Tensor, targets: Tensor,\n",
    "          loss: Loss = MeanSquareError(), \n",
    "          optimizer: Optimizer = SGD(),\n",
    "          iterator: DataIterator = BatchIterator(),\n",
    "          num_epochs: int = 5000) -> None:\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in iterator(inputs, targets):\n",
    "            # Write here the various steps (in order) needed \n",
    "            # at each epoch\n",
    "            batch_pred = net.forward(batch.inputs)\n",
    "            epoch_loss += loss.loss(batch_pred, batch.targets)\n",
    "            grads = loss.grad(batch_pred, batch.targets)\n",
    "            net.backward(grads)\n",
    "            optimizer.step(net)\n",
    "        # Print status every 50 iterations\n",
    "        if epoch % 50 == 0:\n",
    "            print(epoch, epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application exercise\n",
    "\n",
    "Now that you have build your own neural network library, let's use it to solve a problem and then put it in application.\n",
    "\n",
    "### XOR\n",
    "\n",
    "Canonical problem in ML as there is not linear way to map the inputs to the output.\n",
    "\n",
    "```\n",
    "[0, 0] => 0  \n",
    "[0, 1] => 1  \n",
    "[1, 0] => 1  \n",
    "[1, 1] => 0  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "def print_xor_results(inputs: Tensor, targets: Tensor, predictions: Tensor) -> None:\n",
    "    print('\\nX => y => y_pred => round(y_pred)')\n",
    "    for x, y, z in zip(inputs, targets, predictions):\n",
    "        print(f'{x} => {y} => {z} => {z.round()}')\n",
    "        \n",
    "def train_xor(net: Optimizer, inputs: Tensor, targets: Tensor, epochs: int = 2000):\n",
    "    train(net, inputs, targets, num_epochs=epochs)\n",
    "    y_pred = net1.forward(inputs)\n",
    "    print_xor_results(inputs, targets, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an attempt at solving the XOR problem using a single linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.0689150503291245\n",
      "50 1.0709828521313474\n",
      "100 1.0172483073449088\n",
      "150 1.004502346303835\n",
      "200 1.0012232369875043\n",
      "250 1.0003392315573345\n",
      "300 1.000095027673406\n",
      "350 1.0000267483149394\n",
      "400 1.0000075463015208\n",
      "450 1.0000021312732843\n",
      "500 1.000000602231979\n",
      "550 1.0000001702125785\n",
      "600 1.0000000481136062\n",
      "650 1.0000000136008762\n",
      "700 1.0000000038448247\n",
      "750 1.000000001086904\n",
      "800 1.0000000003072615\n",
      "850 1.0000000000868612\n",
      "900 1.0000000000245552\n",
      "950 1.0000000000069418\n",
      "1000 1.0000000000019624\n",
      "1050 1.0000000000005547\n",
      "1100 1.0000000000001568\n",
      "1150 1.0000000000000442\n",
      "1200 1.0000000000000124\n",
      "1250 1.0000000000000036\n",
      "1300 1.0000000000000009\n",
      "1350 1.0000000000000002\n",
      "1400 1.0000000000000002\n",
      "1450 1.0\n",
      "1500 1.0\n",
      "1550 1.0\n",
      "1600 1.0\n",
      "1650 1.0\n",
      "1700 0.9999999999999999\n",
      "1750 1.0\n",
      "1800 1.0\n",
      "1850 1.0\n",
      "1900 1.0\n",
      "1950 1.0\n",
      "\n",
      "X => y => y_pred => round(y_pred)\n",
      "[0 0] => [0] => [0.5] => [1.]\n",
      "[1 0] => [1] => [0.5] => [1.]\n",
      "[0 1] => [1] => [0.5] => [1.]\n",
      "[1 1] => [0] => [0.5] => [0.]\n"
     ]
    }
   ],
   "source": [
    "net1 = NeuralNet([\n",
    "    Linear(input_size=2, output_size=1),\n",
    "])\n",
    "\n",
    "train_xor(net1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not work, as expected since XOR is a typical non-linear problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #6 - solve XOR with a Neural Net\n",
    "\n",
    "Write a more advanced neural net (using additional linear and activation layers) until the predictions match the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.0464083049246473\n",
      "50 0.5588488755947275\n",
      "100 0.3530527970198266\n",
      "150 0.21608251310765006\n",
      "200 0.12777290955629686\n",
      "250 0.07339733585567376\n",
      "300 0.04097060211774631\n",
      "350 0.02225966133252475\n",
      "400 0.011813691461479658\n",
      "450 0.0061516802240413546\n",
      "500 0.0031564491807326706\n",
      "550 0.0016017340690579855\n",
      "600 0.0008061925416301813\n",
      "650 0.00040338168360640945\n",
      "700 0.00020097707728772884\n",
      "750 9.982952789630974e-05\n",
      "800 4.948068611946886e-05\n",
      "850 2.448782034535731e-05\n",
      "900 1.2105895911868244e-05\n",
      "950 5.980178055187882e-06\n",
      "1000 2.952563206168591e-06\n",
      "1050 1.4572061873788667e-06\n",
      "1100 7.189985206722174e-07\n",
      "1150 3.5469436502879204e-07\n",
      "1200 1.7495400026026783e-07\n",
      "1250 8.628864083025046e-08\n",
      "1300 4.25554705401538e-08\n",
      "1350 2.098637898290195e-08\n",
      "1400 1.0349177176883692e-08\n",
      "1450 5.10345675105918e-09\n",
      "1500 2.5166119093083055e-09\n",
      "1550 1.2409756940045327e-09\n",
      "1600 6.119373282828312e-10\n",
      "1650 3.017506811574992e-10\n",
      "1700 1.4879485571829762e-10\n",
      "1750 7.337133337642976e-11\n",
      "1800 3.6179627418341434e-11\n",
      "1850 1.784026065611403e-11\n",
      "1900 8.797066969192508e-12\n",
      "1950 4.337847880382844e-12\n",
      "\n",
      "X => y => y_pred => round(y_pred)\n",
      "[0 0] => [0] => [0.5] => [1.]\n",
      "[1 0] => [1] => [0.5] => [1.]\n",
      "[0 1] => [1] => [0.5] => [1.]\n",
      "[1 1] => [0] => [0.5] => [0.]\n"
     ]
    }
   ],
   "source": [
    "# Write a deeper neural net and see the results\n",
    "net2 = NeuralNet([\n",
    "    # Add the layers here\n",
    "    Linear(input_size=2, output_size=2),\n",
    "    Tanh(),\n",
    "    Linear(input_size=2, output_size=1)\n",
    "])\n",
    "\n",
    "train_xor(net2, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #7 - write the same model using Keras\n",
    "\n",
    "#### Keras installation\n",
    "\n",
    "In a terminal enter the following command to install Keras\n",
    "```\n",
    "conda install --name adaix keras\n",
    "```\n",
    "\n",
    "#### Task\n",
    "Based on the examples given in the [neural network part](https://aboucaud.github.io/adaix-ml-tutorial/slides/hands-on-deep-learning/#39) of the lecture, as well as the section on loss and optimizers, write the exact same example using the Keras library.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X => y => y_pred => round(y_pred)\n",
      "[0 0] => [0] => [2.1457672e-06] => [0.]\n",
      "[1 0] => [1] => [0.9999971] => [1.]\n",
      "[0 1] => [1] => [0.99999684] => [1.]\n",
      "[1 1] => [0] => [1.5497208e-06] => [0.]\n"
     ]
    }
   ],
   "source": [
    "# Write down the keras model below\n",
    "#---------------------------------\n",
    "# Star by the necessary imports\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.metrics import mean_squared_error\n",
    "\n",
    "# Write the model architecture\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(2, input_dim=2))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "opt = SGD(lr=0.05)\n",
    "\n",
    "model.compile(optimizer=opt, loss=mean_squared_error)\n",
    "\n",
    "# Train the model (no validation_split required here)\n",
    "\n",
    "model.fit(X, y, epochs=3000, verbose=0)\n",
    "\n",
    "#---------------------------------\n",
    "\n",
    "# Once trained, this will then predict the values (equivalent of `.forward()`)\n",
    "y_pred_keras = model.predict(X)\n",
    "\n",
    "# And print the results\n",
    "print_xor_results(X, y, y_pred_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star-galaxy separation\n",
    "\n",
    "Use the neural net library you've just created to perform the star-galaxy classification you worked on this morning (cf. [notebook](star-galaxy_classification.ipynb)) using Neural Networks !\n",
    "\n",
    "To help you, here is a helper function that will reload the data and perform the preprocessing for you (you're welcome :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../code/load_star_gal_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_star_gal_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "The idea and the code for this tutorial have been for the most part inspired by the video \"Deep Learning Madness\" https://youtu.be/o64FV-ez6Gw by [Joel Grus](https://twitter.com/joelgrus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
